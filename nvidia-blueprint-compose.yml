version: '3.8'

services:
  hana-ai-backend:
    build:
      context: .
      dockerfile: Dockerfile.nvidia
    image: hana-ai-toolkit-t4:${TAG:-latest}
    container_name: hana-ai-toolkit-t4-backend
    env_file:
      - nvidia-blueprint-environment.env
    environment:
      # Basic API settings
      - API_HOST=0.0.0.0
      - API_PORT=${API_PORT:-8000}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LOG_FORMAT=${LOG_FORMAT:-json}
      - DEPLOYMENT_PLATFORM=nvidia
      
      # T4 GPU optimization settings
      - ENABLE_GPU_ACCELERATION=true
      - ENABLE_TENSORRT=true
      - T4_GPU_FP16_MODE=true
      - T4_GPU_INT8_MODE=true
      - T4_GPU_MAX_WORKSPACE_SIZE=4294967296
      - T4_GPU_MEMORY_FRACTION=0.8
      - T4_GPU_OPTIMIZATION_LEVEL=4
      - PRECISION=fp16
      - CUDA_VISIBLE_DEVICES=0
      - GPU_TYPE=t4
      - T4_OPTIMIZED=true
      
      # Advanced quantization
      - ENABLE_GPTQ=true
      - ENABLE_AWQ=true
      - QUANTIZATION_BIT_WIDTH=4
      - ENABLE_FP8=false
      - ENABLE_FLASH_ATTENTION_2=true
      - ENABLE_KERNEL_FUSION=true
      
      # Adaptive batch sizing
      - ENABLE_ADAPTIVE_BATCH=true
      - ADAPTIVE_BATCH_MIN=1
      - ADAPTIVE_BATCH_MAX=128
      - ADAPTIVE_BATCH_DEFAULT=32
      - ADAPTIVE_BATCH_BENCHMARK_INTERVAL=3600
      - ADAPTIVE_BATCH_CACHE_TTL=300
      
      # Cache settings
      - QUANTIZATION_CACHE_DIR=/tmp/quantization_cache
      - TENSORRT_CACHE_PATH=/app/tensorrt_cache
      
      # Feature flags
      - ENABLE_ALGORITHM_TRANSITIONS=true
      - ENABLE_GESTURE_SUPPORT=true
      - ENABLE_TYPOGRAPHY_PRINCIPLES=true
      
      # Performance monitoring
      - PROMETHEUS_ENABLED=true
      - PROMETHEUS_PORT=9090
      - RATE_LIMIT_PER_MINUTE=100
      - CONNECTION_POOL_SIZE=20
      - DEFAULT_TIMEOUT=60
      
      # Security settings
      - AUTH_REQUIRED=${AUTH_REQUIRED:-false}
      - API_KEYS=${API_KEYS:-}
      - CORS_ORIGINS=*
      - ENFORCE_HTTPS=${ENFORCE_HTTPS:-false}
      
      # Logging settings
      - LOG_REQUESTS=true
      - LOG_RESPONSES=false
      - LOG_PERFORMANCE=true
      - DETAILED_ERROR_RESPONSES=${DETAILED_ERROR_RESPONSES:-true}
    ports:
      - "${API_PORT:-8000}:8000"
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./api:/app/api
      - ./src:/app/src
      - ./tests:/app/tests
      - ./test_results:/app/test_results
      - quantization-cache:/tmp/quantization_cache
      - tensorrt-cache:/app/tensorrt_cache
      - model-cache:/app/model_cache
      - api-logs:/app/logs
    restart: unless-stopped
    networks:
      - hana-ai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # NVIDIA DCGM Exporter for GPU metrics
  dcgm-exporter:
    image: nvcr.io/nvidia/k8s/dcgm-exporter:3.1.7-3.1.4-ubuntu20.04
    container_name: dcgm-exporter
    ports:
      - "9400:9400"
    restart: unless-stopped
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    networks:
      - hana-ai-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  prometheus:
    image: prom/prometheus:latest
    container_name: hana-ai-prometheus
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./prometheus/t4_gpu_alert_rules.yml:/etc/prometheus/t4_gpu_alert_rules.yml
      - ./prometheus/alertmanager.yml:/etc/prometheus/alertmanager.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    ports:
      - "${PROMETHEUS_WEB_PORT:-9091}:9090"
    restart: unless-stopped
    networks:
      - hana-ai-network
    depends_on:
      - hana-ai-backend
      - dcgm-exporter
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Alertmanager for alert management and notification
  alertmanager:
    image: prom/alertmanager:latest
    container_name: hana-ai-alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./prometheus/alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager-data:/alertmanager
    command:
      - --config.file=/etc/alertmanager/alertmanager.yml
      - --storage.path=/alertmanager
    restart: unless-stopped
    networks:
      - hana-ai-network
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  grafana:
    image: grafana/grafana:latest
    container_name: hana-ai-grafana
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning
      - ./deployment/nvidia-t4/grafana/t4-gpu-dashboard.json:/etc/grafana/provisioning/dashboards/t4-gpu-dashboard.json
      - grafana-data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-clock-panel,natel-discrete-panel
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Viewer
      - GF_DASHBOARDS_MIN_REFRESH_INTERVAL=5s
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    restart: unless-stopped
    networks:
      - hana-ai-network
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  nginx:
    image: nginx:alpine
    container_name: hana-ai-nginx
    ports:
      - "${NGINX_PORT:-80}:80"
      - "${NGINX_SSL_PORT:-443}:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./frontend:/usr/share/nginx/html:ro
    environment:
      - API_URL=http://hana-ai-backend:8000
    restart: unless-stopped
    networks:
      - hana-ai-network
    depends_on:
      - hana-ai-backend
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:80"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s

networks:
  hana-ai-network:
    driver: bridge

volumes:
  prometheus-data:
  grafana-data:
  alertmanager-data:
  quantization-cache:
  tensorrt-cache:
  model-cache:
  api-logs: