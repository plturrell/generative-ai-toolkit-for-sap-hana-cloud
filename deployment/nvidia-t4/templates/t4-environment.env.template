# API Server Settings
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=INFO
LOG_FORMAT=json

# Security Settings
AUTH_REQUIRED=true
CORS_ORIGINS=*
ENFORCE_HTTPS=false
RESTRICT_EXTERNAL_CALLS=true

# Rate Limiting Settings
RATE_LIMIT_PER_MINUTE=100

# NVIDIA GPU Optimization Settings - Basic
ENABLE_GPU_ACCELERATION=true
NVIDIA_VISIBLE_DEVICES=all
NVIDIA_DRIVER_CAPABILITIES=compute,utility
CUDA_MEMORY_FRACTION=0.8

# Deployment Configuration
DEPLOYMENT_MODE=api_only
DEPLOYMENT_PLATFORM=nvidia

# NVIDIA GPU Optimization Settings - Advanced
NVIDIA_CUDA_DEVICE_ORDER=PCI_BUS_ID
NVIDIA_CUDA_VISIBLE_DEVICES=0
NVIDIA_TF32_OVERRIDE=0  # Disable TF32 for T4 (Turing architecture doesn't support it)
NVIDIA_CUDA_CACHE_MAXSIZE=2147483648
NVIDIA_CUDA_CACHE_PATH=/tmp/cuda-cache

# Multi-GPU Distribution Settings (if you have multiple T4s)
MULTI_GPU_STRATEGY=data_parallel
ENABLE_TENSOR_PARALLELISM=false  # T4 works better with data parallelism
ENABLE_PIPELINE_PARALLELISM=false
GPU_BATCH_SIZE_OPTIMIZATION=true

# Advanced Kernel Optimization Settings
ENABLE_CUDA_GRAPHS=true
ENABLE_KERNEL_FUSION=true
ENABLE_FLASH_ATTENTION=true
CHECKPOINT_ACTIVATIONS=true

# TensorRT Optimization Settings - T4 Specific
ENABLE_TENSORRT=true
TENSORRT_CACHE_DIR=/tmp/tensorrt_engines
TENSORRT_WORKSPACE_SIZE_MB=1024  # 1GB workspace size
TENSORRT_PRECISION=fp16  # T4 has good FP16 support
TENSORRT_MAX_BATCH_SIZE=16  # Lower for T4 compared to A100/H100
TENSORRT_BUILDER_OPTIMIZATION_LEVEL=3

# Memory Settings
ENABLE_MEMORY=true
MEMORY_EXPIRATION_SECONDS=3600

# Performance Settings
CONNECTION_POOL_SIZE=10
REQUEST_TIMEOUT_SECONDS=300
MAX_REQUEST_SIZE_MB=10

# Monitoring Settings
PROMETHEUS_ENABLED=true
PROMETHEUS_PORT=9090

# Cache Settings
ENABLE_CACHING=true
CACHE_TTL_SECONDS=300  # 5 minutes