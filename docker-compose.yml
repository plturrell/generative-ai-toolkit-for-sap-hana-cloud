version: '3.8'

services:
  # API service with T4 GPU acceleration
  api:
    build:
      context: .
      dockerfile: Dockerfile.nvidia
    ports:
      - "8000:8000"
      - "9090:9090"
    restart: unless-stopped
    environment:
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - LOG_LEVEL=INFO
      - LOG_FORMAT=json
      # T4 GPU optimization settings
      - ENABLE_GPU_ACCELERATION=true
      - ENABLE_TENSORRT=true
      - GPU_MEMORY_FRACTION=0.8
      - PRECISION=fp16
      - BATCH_SIZE=32
      - MAX_SEQUENCE_LENGTH=512
      # Auto-tuning settings
      - ENABLE_AUTOTUNING=true
      - AUTOTUNER_CONFIG_PATH=/app/config/t4_autotuner.json
      - AUTOTUNER_EXPLORATION_RATE=0.1
      - AUTOTUNER_UPDATE_INTERVAL=300
      # Authentication settings
      - AUTH_REQUIRED=false
      - API_KEYS=
      # Performance settings
      - RATE_LIMIT_PER_MINUTE=100
      - CONNECTION_POOL_SIZE=20
      - DEFAULT_TIMEOUT=60
      - PROMETHEUS_ENABLED=true
      - PROMETHEUS_PORT=9090
      # CORS settings
      - CORS_ORIGINS=*
      - ENFORCE_HTTPS=false
      # Logging settings
      - LOG_REQUESTS=true
      - LOG_RESPONSES=false
      - LOG_PERFORMANCE=true
      - LOG_GPU_METRICS=true
      - DETAILED_ERROR_RESPONSES=true
    volumes:
      - ./api:/app/api
      - ./src:/app/src
      - ./tests:/app/tests
      - ./test_results:/app/test_results
      - tensorrt_cache:/app/tensorrt_cache
      - autotuner_data:/app/config
      - api_logs:/app/logs
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  
  # Frontend service with Nginx
  frontend:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - api
    volumes:
      - ./frontend:/usr/share/nginx/html
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
    restart: unless-stopped
  
  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus
    ports:
      - "9091:9090"  # Map to different port to avoid conflict with API metrics
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./prometheus/t4_gpu_alert_rules.yml:/etc/prometheus/t4_gpu_alert_rules.yml
      - prometheus_data:/prometheus
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus
      - --web.console.libraries=/usr/share/prometheus/console_libraries
      - --web.console.templates=/usr/share/prometheus/consoles
      - --web.enable-lifecycle  # Enable runtime reloading of configuration
    depends_on:
      - api
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  
  # DCGM Exporter for T4-specific GPU metrics
  dcgm-exporter:
    image: nvidia/dcgm-exporter:latest
    ports:
      - "9400:9400"
    environment:
      - DCGM_EXPORTER_COLLECTORS=dcp,health,nvlink
      - DCGM_EXPORTER_INTERVAL=1000
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  
  # Grafana for metrics visualization
  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning
      - ./deployment/nvidia-t4/grafana/t4-gpu-dashboard.json:/etc/grafana/provisioning/dashboards/t4-gpu-dashboard.json
      - grafana_data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-clock-panel,natel-discrete-panel
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Viewer
      - GF_DASHBOARDS_MIN_REFRESH_INTERVAL=5s
    depends_on:
      - prometheus
      - dcgm-exporter
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

# Alertmanager for alert management and notification
  alertmanager:
    image: prom/alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./prometheus/alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager_data:/alertmanager
    command:
      - --config.file=/etc/alertmanager/alertmanager.yml
      - --storage.path=/alertmanager
    restart: unless-stopped
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Dynamic threshold adapter service
  threshold-adapter:
    build:
      context: .
      dockerfile: Dockerfile.nvidia
    command: ["python", "-m", "api.prometheus_dynamic_thresholds"]
    volumes:
      - ./api:/app/api
      - ./prometheus:/app/prometheus
      - threshold_data:/app/data
    environment:
      - PROMETHEUS_URL=http://prometheus:9090
      - RULES_FILE=/app/prometheus/t4_gpu_alert_rules.yml
      - CONFIG_FILE=/app/data/dynamic_thresholds.json
      - LOG_LEVEL=INFO
      - UPDATE_INTERVAL=3600
    depends_on:
      - prometheus
    restart: unless-stopped

  # Benchmark metrics collector
  benchmark-collector:
    build:
      context: .
      dockerfile: Dockerfile.nvidia
    command: ["python", "-m", "api.dynamic_benchmarks"]
    volumes:
      - ./api:/app/api
      - ./ngc-blueprint.json:/app/ngc-blueprint.json
      - benchmark_data:/app/data
    environment:
      - BLUEPRINT_FILE=/app/ngc-blueprint.json
      - METRICS_FILE=/app/data/ngc_benchmark_metrics.json
      - LOG_LEVEL=INFO
      - UPDATE_INTERVAL=3600
    depends_on:
      - api
    restart: unless-stopped

volumes:
  api_logs:
  tensorrt_cache:
  prometheus_data:
  grafana_data:
  alertmanager_data:
  autotuner_data:
  threshold_data:
  benchmark_data: